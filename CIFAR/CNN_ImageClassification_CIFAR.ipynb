{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification with Convolutional Neural Networks, CIFAR-10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot graphs inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the CIFAR-10 dataset\n",
    "\n",
    "More information on the dataset can be found here: https://www.cs.toronto.edu/~kriz/cifar.html\n",
    "\n",
    "The file is 17MB so this might take a while\n",
    "\n",
    "The dataset is broken into batches to prevent your machine from running out of memory. The CIFAR-10 dataset consists of 5 batches, named data_batch_1, data_batch_2, etc.. Each batch contains the labels and images that are one of the following:\n",
    "\n",
    "* 0 - airplane\n",
    "* 1 - automobile\n",
    "* 2 - bird\n",
    "* 3 - cat\n",
    "* 4 - deer\n",
    "* 5 - dog\n",
    "* 6 - frog\n",
    "* 7 - horse\n",
    "* 8 - ship\n",
    "* 9 - truck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('cifar-10/cifar-10-python.tar.gz', <http.client.HTTPMessage at 0x11dee29b0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from six.moves import urllib\n",
    "\n",
    "CIFAR_10_FILE_NAME = 'cifar-10/cifar-10-python.tar.gz'\n",
    "\n",
    "urllib.request.urlretrieve(\n",
    "    'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "    CIFAR_10_FILE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Untar and unzip the files\n",
    "\n",
    "* The extracted files (one for each batch) are placed in the folder *cifar-10-batches-py/* under your current working directory \n",
    "* Each file is named *data_batch_1, data_batch_2* etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tarfile\n",
    "\n",
    "with tarfile.open('cifar-10/cifar-10-python.tar.gz') as tar:\n",
    "    tar.extractall()\n",
    "    tar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and pre-process files\n",
    "\n",
    "* Access the image and the labels from a single batch specified by id (1-5)\n",
    "* Reshape the images, the images are **fed to the convolutional layer as a 4-D tensor**, notice that the reshape has the channels at axis index 1 \n",
    "* Transpose the axes of the reshaped image to be in this form: *[batch_size, height, width, channels]*, **channels should be the last axis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "CIFAR10_DATASET_FOLDER = \"cifar-10-batches-py\"\n",
    "\n",
    "def load_cifar10_batch(batch_id):\n",
    "    with open(CIFAR10_DATASET_FOLDER + '/data_batch_' + str(batch_id), mode='rb') as file:\n",
    "        batch = pickle.load(file, encoding='bytes')\n",
    "        \n",
    "    features = batch[b'data'].reshape((len(batch[b'data']), 3, 32, 32)).transpose(0, 2, 3, 1)\n",
    "    labels = batch[b'labels']\n",
    "\n",
    "    return features, labels\n",
    "\n",
    "features, labels = load_cifar10_batch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features) #10000 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions to display images as well as labels\n",
    "\n",
    "* Map the integer labels to the actual labels for display\n",
    "* Plot the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lookup = ['airplane', \n",
    "          'automobile', \n",
    "          'bird', \n",
    "          'cat', \n",
    "          'deer', \n",
    "          'dog', \n",
    "          'frog', \n",
    "          'horse', \n",
    "          'ship', \n",
    "          'truck']\n",
    "\n",
    "def display_feature_label(features, labels, index):\n",
    "    if index >= len(features):\n",
    "        raise Error(\"index out of range\")\n",
    "        \n",
    "    print(\"Label: \", lookup[labels[index]])\n",
    "    plt.imshow(features[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label:  automobile\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_feature_label(features, labels, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access the *training* data and the corresponding labels\n",
    "\n",
    "Each batch in the CIFAR-10 dataset has randomly picked images, so the images come pre-shuffled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training images:  8000\n",
      "Training labels:  8000\n"
     ]
    }
   ],
   "source": [
    "train_size = int(len(features) * 0.8)\n",
    "\n",
    "training_images = features[:train_size,:,:]\n",
    "\n",
    "training_labels = labels[:train_size]\n",
    "\n",
    "print(\"Training images: \", len(training_images))\n",
    "print(\"Training labels: \", len(training_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access the *test* data and the corresponding labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test images:  2000\n",
      "Test labels:  2000\n"
     ]
    }
   ],
   "source": [
    "test_images = features[train_size:,:,:]\n",
    "\n",
    "test_labels = labels[train_size:]\n",
    "\n",
    "print(\"Test images: \", len(test_images))\n",
    "print(\"Test labels: \", len(test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The CIFAR-10 dataset has color images\n",
    "\n",
    "* Each image is of size 32x32\n",
    "* The image is RGB so has 3 channels, and requires 3 numbers to represent each pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "height = 32\n",
    "width = 32\n",
    "channels = 3\n",
    "n_inputs = height * width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Placeholders for training data and labels\n",
    "\n",
    "* The training dataset placeholder can have any number of instances and each instance is an array of 32x32 pixels (we've already reshaped the data earlier)\n",
    "* The images are fed to the convolutional layer as a 4D tensor *[batch_size, height, width, channels]*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[None,  height, width, channels], name=\"X\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add a dropout layer to avoid overfitting the training data\n",
    "\n",
    "* The training flag is set to False during prediction and is True while training (dropout is applied only in the training phase)\n",
    "* The dropout_rate indicates the chances that a neuron is turned off during training\n",
    "* Here 30% of the neurons will be dropped off at somepoint in the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dropout_rate = 0.3\n",
    "\n",
    "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "X_drop = tf.layers.dropout(X, dropout_rate, training=training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = tf.placeholder(tf.int32, shape=[None], name=\"y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural network design\n",
    "\n",
    "* 2 convolutional layers\n",
    "* 1 max pooling layer\n",
    "* 1 convolutional layer\n",
    "* 1 max pooling layer\n",
    "* 2 fully connected layers\n",
    "* Output logits layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv1 = tf.layers.conv2d(X_drop, filters=32,\n",
    "                         kernel_size=3,\n",
    "                         strides=1, padding=\"SAME\",\n",
    "                         activation=tf.nn.relu, name=\"conv1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv2 = tf.layers.conv2d(conv1, filters=64, \n",
    "                         kernel_size=3,\n",
    "                         strides=2, padding=\"SAME\",\n",
    "                         activation=tf.nn.relu, name=\"conv2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(16), Dimension(16), Dimension(64)])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling reduces the size of the image\n",
    "\n",
    "The pooled image is only 1/4th the size of the original image with this kernel size and stride"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pool3 = tf.nn.max_pool(conv2,\n",
    "                       ksize=[1, 2, 2, 1],\n",
    "                       strides=[1, 2, 2, 1],\n",
    "                       padding=\"VALID\") #no zero padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv4 = tf.layers.conv2d(pool3, filters=128, \n",
    "                         kernel_size=4,\n",
    "                         strides=3, padding=\"SAME\",\n",
    "                         activation=tf.nn.relu, name=\"conv4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshape the pooled layer to be a 1-D vector (flatten it) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pool5 = tf.nn.max_pool(conv4,\n",
    "                       ksize=[1, 2, 2, 1],\n",
    "                       strides=[1, 1, 1, 1],\n",
    "                       padding=\"VALID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(2), Dimension(2), Dimension(128)])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool5.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pool5_flat = tf.reshape(pool5, shape=[-1, 128 * 2 * 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullyconn1 = tf.layers.dense(pool5_flat, 128,\n",
    "                             activation=tf.nn.relu, name=\"fc1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fullyconn2 = tf.layers.dense(fullyconn1, 64,\n",
    "                             activation=tf.nn.relu, name=\"fc2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There are 10 possible classifications in the CIFAR-10 dataset\n",
    "\n",
    "The number of outputs of the logits layer should be 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logits = tf.layers.dense(fullyconn2, 10, name=\"output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The final output layer with softmax activation\n",
    "\n",
    "The *tf.nn.sparse_softmax_cross_entropy_with_logits* will apply the softmax activation as well as calculate the cross-entropy as our cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits,\n",
    "                                                          labels=y)\n",
    "loss = tf.reduce_mean(xentropy)\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check correctness and accuracy of the prediction\n",
    "\n",
    "* Check whether the highest probability output in logits is equal to the y-label\n",
    "* Check the accuracy across all predictions (How many predictions did we get right?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct = tf.nn.in_top_k(logits, y, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up a helper method to access training data in batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_next_batch(features, labels, train_size, batch_index, batch_size):\n",
    "    training_images = features[:train_size,:,:]\n",
    "    training_labels = labels[:train_size]\n",
    "    \n",
    "    test_images = features[train_size:,:,:]\n",
    "    test_labels = labels[train_size:]\n",
    "    \n",
    "    start_index = batch_index * batch_size\n",
    "    end_index = start_index + batch_size\n",
    "\n",
    "    return features[start_index:end_index,:,:], labels[start_index:end_index], test_images, test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and evaluate the model\n",
    "\n",
    "* For smaller training data you'll find that the model performs poorly, it improves as you increase the size of the training data (use all batches)\n",
    "* Ensure that dropout is enabled during training to avoid overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train accuracy: 0.234375 Test accuracy: 0.304\n",
      "1 Train accuracy: 0.3125 Test accuracy: 0.343\n",
      "2 Train accuracy: 0.351562 Test accuracy: 0.3575\n",
      "3 Train accuracy: 0.359375 Test accuracy: 0.3815\n",
      "4 Train accuracy: 0.4375 Test accuracy: 0.448\n",
      "5 Train accuracy: 0.429688 Test accuracy: 0.453\n",
      "6 Train accuracy: 0.492188 Test accuracy: 0.482\n",
      "7 Train accuracy: 0.5 Test accuracy: 0.492\n",
      "8 Train accuracy: 0.515625 Test accuracy: 0.4975\n",
      "9 Train accuracy: 0.507812 Test accuracy: 0.528\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "batch_size = 128\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "#         batch_index = 0\n",
    "        # Add this in when we want to run the training on all batches in CIFAR-10\n",
    "        for batch_id in range(1, 6):\n",
    "            batch_index = 0\n",
    "            \n",
    "            features, labels = load_cifar10_batch(batch_id)\n",
    "            train_size = int(len(features) * 0.8)\n",
    "            \n",
    "            for iteration in range(train_size // batch_size):\n",
    "                X_batch, y_batch, test_images, test_labels = get_next_batch(features, \n",
    "                                                                            labels, \n",
    "                                                                            train_size, \n",
    "                                                                            batch_index,\n",
    "                                                                            batch_size)\n",
    "                batch_index += 1\n",
    "\n",
    "                sess.run(training_op, feed_dict={X: X_batch, y: y_batch, training: True})\n",
    "\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_test = accuracy.eval(feed_dict={X: test_images, y: test_labels})\n",
    "        print(epoch, \"Train accuracy:\", acc_train, \"Test accuracy:\", acc_test)\n",
    "\n",
    "        save_path = saver.save(sess, \"./my_mnist_model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
