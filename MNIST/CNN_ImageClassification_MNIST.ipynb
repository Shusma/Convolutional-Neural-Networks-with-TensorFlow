{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification with Convolutional Neural Networks\n",
    "\n",
    "Original code at: https://github.com/ageron/handson-ml/blob/master/13_convolutional_neural_networks.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot graphs inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and extract the MNIST libraries\n",
    "\n",
    "The original site where this dataset is available: http://yann.lecun.com/exdb/mnist/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0819 23:31:07.053544 140735651656576 deprecation.py:323] From <ipython-input-4-bc58aa514b3f>:5: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "W0819 23:31:07.054604 140735651656576 deprecation.py:323] From /Users/shusma/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "W0819 23:31:07.056197 140735651656576 deprecation.py:323] From /Users/shusma/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting mnist_data/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0819 23:31:07.302754 140735651656576 deprecation.py:323] From /Users/shusma/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "W0819 23:31:07.349221 140735651656576 deprecation.py:323] From /Users/shusma/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting mnist_data/train-labels-idx1-ubyte.gz\n",
      "Extracting mnist_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting mnist_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# Store the MNIST data in mnist_data/\n",
    "mnist = input_data.read_data_sets(\"mnist_data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function to display one digit image\n",
    "\n",
    "Reshape the data from 1-D array to a 2-D array of 28x28 pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def display_digit(digit):\n",
    "    plt.imshow(digit.reshape(28, 28), cmap=\"Greys\", interpolation='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access the training and test data and the corresponding labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_digits, training_labels = mnist.train.next_batch(10000)\n",
    "test_digits, test_labels = mnist.test.next_batch(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAN80lEQVR4nO3df6xU9ZnH8c8jAgmUKPReDbGysM1NrGlSaEayUdOwabaiJmJNuoEQoonJbSJGTEpcU6NVQUPsdol/rFW6JbCbrtiEYsEYF4P4o3+IjIZVkLiyeJdSES5i6K0JVuHpH/e43uLMd4ZzzswZeN6vZDIz55lzzpPJ/dwzc86c8zV3F4Bz33lVNwCgOwg7EARhB4Ig7EAQhB0I4vxurqyvr89nzpzZzVUCoQwNDeno0aPWqFYo7GY2X9KjksZJ+jd3X5V6/cyZM1Wv14usEkBCrVZrWsv9Md7Mxkn6V0nXSrpc0iIzuzzv8gB0VpHv7HMl7XP3/e7+Z0kbJC0opy0AZSsS9ksk/X7M84PZtL9iZoNmVjez+vDwcIHVASiiSNgb7QT40m9v3X2Nu9fcvdbf319gdQCKKBL2g5IuHfP8a5LeL9YOgE4pEvadkgbMbJaZTZC0UNLmctoCULbch97c/TMzu13Sf2n00Ntad99TWmcoxcmTJ5P1Bx98MFlfsWJFsn7q1Kkz7gnVKHSc3d2flfRsSb0A6CB+LgsEQdiBIAg7EARhB4Ig7EAQhB0Ioqvns6P71q1bl6yvXLkyWTdreGo0zkJs2YEgCDsQBGEHgiDsQBCEHQiCsANBcOjtHJA6jXXLli2Fln3FFVcUmh+9gy07EARhB4Ig7EAQhB0IgrADQRB2IAjCDgTBcfZzwODgYNNaq+Psc+bMSdZfeOGFXD2h97BlB4Ig7EAQhB0IgrADQRB2IAjCDgRB2IEgOM5+FtixY0eyvmHDhtzLnj9/frI+adKk3MtGbykUdjMbkjQi6aSkz9y9VkZTAMpXxpb97939aAnLAdBBfGcHgigadpe01cxeN7OGP9A2s0Ezq5tZfXh4uODqAORVNOxXufu3JV0raamZfef0F7j7GnevuXutv7+/4OoA5FUo7O7+fnZ/RNImSXPLaApA+XKH3cwmm9mUzx9L+p6k3WU1BqBcRfbGXyxpUzak7/mS/tPdnyulq2A++OCDZP2aa65J1k+cONG0tnjx4uS89957b7KOc0fusLv7fknfKrEXAB3EoTcgCMIOBEHYgSAIOxAEYQeC4BTXLnD3ZP3FF19M1kdGRpL1yZMnN62tXLkyOe/EiROTdZw72LIDQRB2IAjCDgRB2IEgCDsQBGEHgiDsQBAcZ++CV155JVlvdRpqK6+99lrT2owZMwotG+cOtuxAEIQdCIKwA0EQdiAIwg4EQdiBIAg7EATH2Utw/PjxZH3p0qWFln/XXXcl6wMDA4WWjxjYsgNBEHYgCMIOBEHYgSAIOxAEYQeCIOxAEBxnL0Gr89HffvvtZP3qq69O1lesWJGsjxs3LlkHpDa27Ga21syOmNnuMdOmmdnzZvZudj+1s20CKKqdj/HrJM0/bdrdkra5+4CkbdlzAD2sZdjd/WVJx06bvEDS+uzxekk3ltwXgJLl3UF3sbsfkqTs/qJmLzSzQTOrm1l9eHg45+oAFNXxvfHuvsbda+5e6+/v7/TqADSRN+yHzWy6JGX3R8prCUAn5A37Zkk3Z49vlvTbctoB0Cktj7Ob2ZOS5knqM7ODkn4iaZWkX5vZrZIOSPpBJ5vsBZ9++mnT2oEDB5Lzthqf/YYbbkjWzz+fn0OguJZ/Re6+qEnpuyX3AqCD+LksEARhB4Ig7EAQhB0IgrADQXBMp0379u1rWtuzZ09y3okTJybr119/fa6eojtx4kSy/t577+Ve9kcffZSsb9++PVlvdfnv8ePHn3FPRbFlB4Ig7EAQhB0IgrADQRB2IAjCDgRB2IEgOM7ept27d7d+URMPPPBAsn7ZZZflXvbZrNVlyl566aVk/b777kvW33nnnTPuqSythum+8MILu9TJF9iyA0EQdiAIwg4EQdiBIAg7EARhB4Ig7EAQHGfPnDp1KlnftGlT7mX39fXlnrfXtXrfHn/88aa15cuXJ+f95JNPcvWExtiyA0EQdiAIwg4EQdiBIAg7EARhB4Ig7EAQHGfPHD9+PFl/6qmnutTJ2eWxxx5L1pctW5Z72bNmzUrW582bl6zfcsstTWurV69Ozvv0008n60uWLEnWJ02alKxXoeWW3czWmtkRM9s9Ztr9ZvYHM9uV3a7rbJsAimrnY/w6SfMbTF/t7rOz27PltgWgbC3D7u4vSzrWhV4AdFCRHXS3m9mb2cf8qc1eZGaDZlY3s3qra44B6Jy8Yf+5pK9Lmi3pkKSfNXuhu69x95q71/r7+3OuDkBRucLu7ofd/aS7n5L0C0lzy20LQNlyhd3Mpo95+n1J+a+zDKArWh5nN7MnJc2T1GdmByX9RNI8M5stySUNSfphB3s86z300EPJeqtriN90001ltnNGWu1nueOOO5J1M2tae+SRR5LzDg4OJutTpkxJ1jdu3Ni09swzzyTnbXUt/yeeeCJZnzBhQrJehZZhd/dFDSb/sgO9AOggfi4LBEHYgSAIOxAEYQeCIOxAEJzimrnggguS9UcffbRprdVpnENDQ8n6okWNDnh8Yf78RuchfeHhhx9uWhsYGEjO++GHHybrV155ZbLeSmpY5VbDGu/bty9Zv+eee5L15557rmlt4sSJyXlbneLaav5exJYdCIKwA0EQdiAIwg4EQdiBIAg7EARhB4Iwd+/aymq1mtfr9a6tr0wjIyNNa62Og7/66qtlt9MzWg3ZfN551W1PFi9e3LS2cuXK5LwzZswou52uqNVqqtfrDc8rZssOBEHYgSAIOxAEYQeCIOxAEIQdCIKwA0FwPnubUpctTp03LUnLly9P1rdu3ZqsHzhwIFmvUupS0UW1Omd84cKFyXrqUtV9fX25ejqbsWUHgiDsQBCEHQiCsANBEHYgCMIOBEHYgSA4zl6CVkMHtxre9+OPP07W9+/fn6xv3ry5aW3Lli3JeXfu3Jmst7JgwYJkvVar5V72bbfdlqxPnTo197IjarllN7NLzWy7me01sz1mtiybPs3Mnjezd7N73nmgh7XzMf4zST9y929I+jtJS83sckl3S9rm7gOStmXPAfSolmF390Pu/kb2eETSXkmXSFogaX32svWSbuxUkwCKO6MddGY2U9IcSTskXezuh6TRfwiSLmoyz6CZ1c2sPjw8XKxbALm1HXYz+4qkjZLudPc/tjufu69x95q71/r7+/P0CKAEbYXdzMZrNOi/cvffZJMPm9n0rD5d0pHOtAigDC0vJW2j5zCul3TM3e8cM/2nkj5091Vmdrekae5+V2pZZ/OlpIGzQepS0u0cZ79K0hJJb5nZrmzajyWtkvRrM7tV0gFJPyijWQCd0TLs7v47Sc2uUPDdctsB0Cn8XBYIgrADQRB2IAjCDgRB2IEgCDsQBGEHgiDsQBCEHQiCsANBEHYgCMIOBEHYgSAIOxAEYQeCIOxAEIQdCIKwA0EQdiAIwg4EQdiBIAg7EARhB4Ig7EAQhB0IgrADQRB2IAjCDgRB2IEgWobdzC41s+1mttfM9pjZsmz6/Wb2BzPbld2u63y7APJqZ3z2zyT9yN3fMLMpkl43s+ez2mp3/+fOtQegLO2Mz35I0qHs8YiZ7ZV0SacbA1CuM/rObmYzJc2RtCObdLuZvWlma81sapN5Bs2sbmb14eHhQs0CyK/tsJvZVyRtlHSnu/9R0s8lfV3SbI1u+X/WaD53X+PuNXev9ff3l9AygDzaCruZjddo0H/l7r+RJHc/7O4n3f2UpF9Imtu5NgEU1c7eeJP0S0l73f1fxkyfPuZl35e0u/z2AJSlnb3xV0laIuktM9uVTfuxpEVmNluSSxqS9MOOdAigFO3sjf+dJGtQerb8dgB0Cr+gA4Ig7EAQhB0IgrADQRB2IAjCDgRB2IEgCDsQBGEHgiDsQBCEHQiCsANBEHYgCMIOBGHu3r2VmQ1L+r8xk/okHe1aA2emV3vr1b4kesurzN7+xt0bXv+tq2H/0srN6u5eq6yBhF7trVf7kugtr271xsd4IAjCDgRRddjXVLz+lF7trVf7kugtr670Vul3dgDdU/WWHUCXEHYgiErCbmbzzewdM9tnZndX0UMzZjZkZm9lw1DXK+5lrZkdMbPdY6ZNM7Pnzezd7L7hGHsV9dYTw3gnhhmv9L2revjzrn9nN7Nxkv5H0j9IOihpp6RF7v52VxtpwsyGJNXcvfIfYJjZdyT9SdK/u/s3s2mPSDrm7quyf5RT3f2feqS3+yX9qephvLPRiqaPHWZc0o2SblGF712ir39UF963KrbscyXtc/f97v5nSRskLaigj57n7i9LOnba5AWS1meP12v0j6XrmvTWE9z9kLu/kT0ekfT5MOOVvneJvrqiirBfIun3Y54fVG+N9+6StprZ62Y2WHUzDVzs7oek0T8eSRdV3M/pWg7j3U2nDTPeM+9dnuHPi6oi7I2Gkuql439Xufu3JV0raWn2cRXtaWsY725pMMx4T8g7/HlRVYT9oKRLxzz/mqT3K+ijIXd/P7s/ImmTem8o6sOfj6Cb3R+puJ//10vDeDcaZlw98N5VOfx5FWHfKWnAzGaZ2QRJCyVtrqCPLzGzydmOE5nZZEnfU+8NRb1Z0s3Z45sl/bbCXv5Krwzj3WyYcVX83lU+/Lm7d/0m6TqN7pH/X0n3VNFDk77+VtJ/Z7c9Vfcm6UmNfqz7VKOfiG6V9FVJ2yS9m91P66He/kPSW5Le1GiwplfU29Ua/Wr4pqRd2e26qt+7RF9ded/4uSwQBL+gA4Ig7EAQhB0IgrADQRB2IAjCDgRB2IEg/gLsLjj9VzWwggAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_digit(training_digits[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9, 6, 1, 8, 5], dtype=uint8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_labels[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensions of MNIST images\n",
    "\n",
    "* Each image is 28x28 pixels\n",
    "* The images are grayscale and have just one channel\n",
    "* The number of inputs is equal to the number of pixels in each image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "height = 28\n",
    "width = 28\n",
    "channels = 1\n",
    "n_inputs = height * width"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up a CNN with 2 convolutional layers and one max pool layer\n",
    "\n",
    "* Specify the number of **feature maps** in each layer, a feature map highlights that area in an image which is most similar to the filter applied\n",
    "* The kernel size indicates the **dimensions of the filter** which is applied to the image. The filter variables are created for you and initialized randomly\n",
    "* The stride is the steps by which the filter moves over the input, the **distance between two receptive fields on the input**\n",
    "* \"SAME\" padding indicates that the convolutional layer **uses zero padding** on the inputs and will consider all inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv1_feature_maps = 32\n",
    "conv1_kernel_size = 3\n",
    "conv1_stride = 1\n",
    "conv1_pad = \"SAME\" # means input is zero padded - edges are included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv2_feature_maps = 64 \n",
    "conv2_kernel_size = 3\n",
    "conv2_stride = 2\n",
    "conv2_pad = \"SAME\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pool3_feature_maps = conv2_feature_maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One fully connected layer\n",
    "\n",
    "* 64 neurons in the layer\n",
    "* 10 outputs corresponding to the digits 0-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_fullyconn1 = 64\n",
    "n_outputs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Placeholders for training data and labels\n",
    "\n",
    "* The training dataset placeholder can have any number of instances and each instance is an array of n_inputs = 28x28 = 784 pixels\n",
    "* The images are fed to the convolutional layer as a 4D tensor *[batch_size, height, width, channels]*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[None, n_inputs], name=\"X\")\n",
    "X_reshaped = tf.reshape(X, shape=[-1, height, width, channels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape for the X placeholder is [None, n_inputs] - the None denotes that we do not know how many images we will read in one batch, the *n_input* represents the no.of inputs ie. the 784 pixels = [*batch_size*, 784]\n",
    "\n",
    "For the CNN we wish our image to have both height and width - therefore we reshape the input tensor - the $-1$ indicates the the first dimension remains the *batch_size*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = tf.placeholder(tf.int32, shape=[None], name=\"y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect the convolutional layers\n",
    "\n",
    "* Each layer uses the ReLU activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0819 23:31:08.180289 140735651656576 deprecation.py:323] From <ipython-input-17-a6590d9f8110>:4: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.keras.layers.Conv2D` instead.\n",
      "W0819 23:31:08.190810 140735651656576 deprecation.py:506] From /Users/shusma/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "conv1 = tf.layers.conv2d(X_reshaped, filters=conv1_feature_maps,\n",
    "                         kernel_size=conv1_kernel_size,\n",
    "                         strides=conv1_stride, padding=conv1_pad,\n",
    "                         activation=tf.nn.relu, name=\"conv1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv2 = tf.layers.conv2d(conv1, filters=conv2_feature_maps, \n",
    "                         kernel_size=conv2_kernel_size,\n",
    "                         strides=conv2_stride, padding=conv2_pad,\n",
    "                         activation=tf.nn.relu, name=\"conv2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(14), Dimension(14), Dimension(64)])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *height* and *width* have been reduced to 14, however the depth is preserved and remains equal to the no. of feature maps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect a max pooling layer\n",
    "\n",
    "* The filter is a 2x2 filter\n",
    "* Kernel size = [*batch_size, height, width, channels*] - TF doesn't allow pooling over multiple instances of the batch, restraining the *batch_size* to be 1.\n",
    "* The stride is 2 both horizontally and vertically\n",
    "* Stride = [*batch_stride,horizontal, vertical, channels*]\n",
    "* This results in an image that is **1/4th the size of the original image**\n",
    "* Reshape the pooled layer to be a **1-D vector (flatten it)**. It now has only 1/4th the number of pixels in each feature map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pool3 = tf.nn.max_pool(conv2,\n",
    "                       ksize=[1, 2, 2, 1],\n",
    "                       strides=[1, 2, 2, 1],\n",
    "                       padding=\"VALID\")\n",
    "\n",
    "pool3_flat = tf.reshape(pool3, shape=[-1, pool3_feature_maps * 7 * 7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(7), Dimension(7), Dimension(64)])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool3.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to feed in this tensor to a fully connected neural network, we have to flatten the results of the pooling layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(3136)])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool3_flat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0819 23:31:08.662863 140735651656576 deprecation.py:323] From <ipython-input-23-3501d0850430>:2: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n"
     ]
    }
   ],
   "source": [
    "fullyconn1 = tf.layers.dense(pool3_flat, n_fullyconn1,\n",
    "                             activation=tf.nn.relu, name=\"fc1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The final output layer with softmax activation\n",
    "\n",
    "Do not apply the softmax activation to this layer. The *tf.nn.sparse_softmax_cross_entropy_with_logits* will apply the softmax activation as well as calculate the cross-entropy as our cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logits = tf.layers.dense(fullyconn1, n_outputs, name=\"output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-entropy as a cost function\n",
    "\n",
    "* $-\\Sigma(Y_{actual} * log[y_{predicted}])$\n",
    "* Use the Adam optimizer which in most cases performs better than the simple gradient descent\n",
    "\n",
    "> **Adam Optimiser**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits,\n",
    "                                                          labels=y)\n",
    "loss = tf.reduce_mean(xentropy)\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check correctness and accuracy of the prediction\n",
    "\n",
    "* Check whether the highest probability output in logits is equal to the y-label - k = 1\n",
    "* Check the accuracy across all predictions (How many predictions did we get right?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct = tf.nn.in_top_k(logits, y, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train with the training data, measure accuracy with test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train accuracy: 0.99 Test accuracy: 0.9768\n",
      "1 Train accuracy: 1.0 Test accuracy: 0.9812\n",
      "2 Train accuracy: 1.0 Test accuracy: 0.9868\n",
      "3 Train accuracy: 1.0 Test accuracy: 0.9882\n",
      "4 Train accuracy: 0.99 Test accuracy: 0.9878\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 5\n",
    "batch_size = 100\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_test = accuracy.eval(feed_dict={X: mnist.test.images, y: mnist.test.labels})\n",
    "        print(epoch, \"Train accuracy:\", acc_train, \"Test accuracy:\", acc_test)\n",
    "\n",
    "        save_path = saver.save(sess, \"./my_mnist_model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
